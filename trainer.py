"""åˆ†å¸ƒå¼è®­ç»ƒå™¨æ¨¡å—"""
import os
from dataclasses import dataclass
from typing import List, Tuple
import torch
import torch.nn as nn
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel as DDP
from torch.amp import autocast, GradScaler
from torch.utils.data import DataLoader
from tqdm import tqdm


@dataclass
class TrainingConfig:
    """è®­ç»ƒé…ç½®"""
    # æ•°æ®ç›¸å…³
    data_dir: str = "./cifar-10-batches-py"
    batch_size: int = 128  # æ¯ä¸ªGPUçš„batch size
    num_workers: int = 4
    
    # æ¨¡å‹ç›¸å…³
    num_classes: int = 10
    dropout_rate: float = 0.5
    
    # è®­ç»ƒç›¸å…³
    epochs: int = 200
    learning_rate: float = 0.1
    momentum: float = 0.9
    weight_decay: float = 5e-4
    
    # åˆ†å¸ƒå¼ç›¸å…³
    use_amp: bool = True  # æ··åˆç²¾åº¦è®­ç»ƒ
    
    # è¾“å‡ºç›¸å…³
    save_dir: str = "./outputs"
    model_name: str = "cifar10_cnn"


class DistributedTrainer:
    """åˆ†å¸ƒå¼è®­ç»ƒå™¨"""
    
    def __init__(
        self,
        model: nn.Module,
        train_loader: DataLoader,
        test_loader: DataLoader,
        config: TrainingConfig,
        rank: int = 0,
        world_size: int = 1
    ):
        """
        Args:
            model: æ¨¡å‹
            train_loader: è®­ç»ƒæ•°æ®åŠ è½½å™¨
            test_loader: æµ‹è¯•æ•°æ®åŠ è½½å™¨
            config: è®­ç»ƒé…ç½®
            rank: å½“å‰è¿›ç¨‹çš„rank
            world_size: æ€»è¿›ç¨‹æ•°
        """
        self.config = config
        self.rank = rank
        self.world_size = world_size
        self.device = torch.device(f'cuda:{rank}' if torch.cuda.is_available() else 'cpu')
        
        # æ¨¡å‹
        self.model = model.to(self.device)
        if world_size > 1:
            self.model = DDP(self.model, device_ids=[rank])
        
        # æ•°æ®åŠ è½½å™¨
        self.train_loader = train_loader
        self.test_loader = test_loader
        
        # æŸå¤±å‡½æ•°
        self.criterion = nn.CrossEntropyLoss()
        
        # ä¼˜åŒ–å™¨
        self.optimizer = torch.optim.SGD(
            self.model.parameters(),
            lr=config.learning_rate,
            momentum=config.momentum,
            weight_decay=config.weight_decay
        )
        
        # å­¦ä¹ ç‡è°ƒåº¦å™¨
        self.scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
            self.optimizer,
            T_max=config.epochs
        )
        
        # æ··åˆç²¾åº¦è®­ç»ƒ
        if config.use_amp and torch.cuda.is_available():
            self.scaler = GradScaler('cuda')
        else:
            self.scaler = None
        
        # è®°å½•
        self.train_losses = []
        self.test_accuracies = []
        self.best_acc = 0.0
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        if rank == 0:
            os.makedirs(config.save_dir, exist_ok=True)
    
    def train_epoch(self, epoch: int) -> float:
        """è®­ç»ƒä¸€ä¸ªepoch"""
        self.model.train()
        total_loss = 0.0
        num_batches = 0
        
        # åªåœ¨ä¸»è¿›ç¨‹æ˜¾ç¤ºè¿›åº¦æ¡
        if self.rank == 0:
            pbar = tqdm(self.train_loader, 
                       desc=f'Epoch {epoch}/{self.config.epochs} [Train]',
                       ncols=100,
                       leave=False)
        else:
            pbar = self.train_loader
        
        for images, labels in pbar:
            images = images.to(self.device)
            labels = labels.to(self.device)
            
            self.optimizer.zero_grad()
            
            # æ··åˆç²¾åº¦è®­ç»ƒ
            if self.config.use_amp and self.scaler is not None:
                with autocast('cuda'):
                    outputs = self.model(images)
                    loss = self.criterion(outputs, labels)
                
                self.scaler.scale(loss).backward()
                self.scaler.step(self.optimizer)
                self.scaler.update()
            else:
                outputs = self.model(images)
                loss = self.criterion(outputs, labels)
                loss.backward()
                self.optimizer.step()
            
            total_loss += loss.item()
            num_batches += 1
            
            # æ›´æ–°è¿›åº¦æ¡æ˜¾ç¤ºå½“å‰loss
            if self.rank == 0:
                pbar.set_postfix({'loss': f'{loss.item():.4f}'})
        
        avg_loss = total_loss / num_batches
        return avg_loss
    
    def evaluate(self, epoch: int) -> float:
        """è¯„ä¼°æ¨¡å‹"""
        self.model.eval()
        correct = 0
        total = 0
        
        # åªåœ¨ä¸»è¿›ç¨‹æ˜¾ç¤ºè¿›åº¦æ¡
        if self.rank == 0:
            pbar = tqdm(self.test_loader,
                       desc=f'Epoch {epoch}/{self.config.epochs} [Eval]',
                       ncols=100,
                       leave=False)
        else:
            pbar = self.test_loader
        
        with torch.no_grad():
            for images, labels in pbar:
                images = images.to(self.device)
                labels = labels.to(self.device)
                
                outputs = self.model(images)
                _, predicted = torch.max(outputs.data, 1)
                
                total += labels.size(0)
                correct += (predicted == labels).sum().item()
                
                # æ›´æ–°è¿›åº¦æ¡æ˜¾ç¤ºå½“å‰å‡†ç¡®ç‡
                if self.rank == 0:
                    current_acc = correct / total
                    pbar.set_postfix({'acc': f'{current_acc:.4f}'})
        
        accuracy = correct / total
        return accuracy
    
    def save_checkpoint(self, epoch: int, is_best: bool = False):
        """ä¿å­˜æ¨¡å‹æ£€æŸ¥ç‚¹"""
        if self.rank != 0:
            return
        
        model_state = self.model.module.state_dict() if self.world_size > 1 else self.model.state_dict()
        
        checkpoint = {
            'epoch': epoch,
            'model_state_dict': model_state,
            'optimizer_state_dict': self.optimizer.state_dict(),
            'scheduler_state_dict': self.scheduler.state_dict(),
            'best_acc': self.best_acc,
            'train_losses': self.train_losses,
            'test_accuracies': self.test_accuracies
        }
        
        # ä¿å­˜æœ€æ–°æ£€æŸ¥ç‚¹
        checkpoint_path = os.path.join(self.config.save_dir, f'{self.config.model_name}_latest.pth')
        torch.save(checkpoint, checkpoint_path)
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if is_best:
            best_path = os.path.join(self.config.save_dir, f'{self.config.model_name}_best.pth')
            torch.save(checkpoint, best_path)
    
    def train(self) -> Tuple[List[float], List[float]]:
        """å®Œæ•´è®­ç»ƒæµç¨‹"""
        if self.rank == 0:
            print("=" * 80)
            print(f"{'CIFAR-10 CNN Training':^80}")
            print("=" * 80)
            print(f"{'Epochs:':<20} {self.config.epochs}")
            print(f"{'Device:':<20} {self.device}")
            print(f"{'World Size:':<20} {self.world_size}")
            print(f"{'Batch Size:':<20} {self.config.batch_size} Ã— {self.world_size} = {self.config.batch_size * self.world_size}")
            print(f"{'Learning Rate:':<20} {self.config.learning_rate}")
            print(f"{'Mixed Precision:':<20} {'Enabled' if self.config.use_amp else 'Disabled'}")
            print("=" * 80)
        
        for epoch in range(1, self.config.epochs + 1):
            # è®­ç»ƒ
            avg_loss = self.train_epoch(epoch)
            self.train_losses.append(avg_loss)
            
            # è¯„ä¼°
            accuracy = self.evaluate(epoch)
            self.test_accuracies.append(accuracy)
            
            # æ›´æ–°å­¦ä¹ ç‡
            current_lr = self.scheduler.get_last_lr()[0]
            self.scheduler.step()
            
            # ä¿å­˜æœ€ä½³æ¨¡å‹
            is_best = accuracy > self.best_acc
            if is_best:
                self.best_acc = accuracy
            
            self.save_checkpoint(epoch, is_best)
            
            # æ‰“å°æ¯è½®è®­ç»ƒä¿¡æ¯
            if self.rank == 0:
                status = "ğŸŒŸ NEW BEST!" if is_best else ""
                print(f"Epoch [{epoch:3d}/{self.config.epochs}] "
                      f"Loss: {avg_loss:.4f} | "
                      f"Acc: {accuracy:.4f} | "
                      f"Best: {self.best_acc:.4f} | "
                      f"LR: {current_lr:.6f} "
                      f"{status}")
        
        if self.rank == 0:
            print("=" * 80)
            print(f"{'Training Completed!':^80}")
            print(f"{'Best Accuracy:':<20} {self.best_acc:.4f}")
            print("=" * 80)
        
        return self.train_losses, self.test_accuracies
