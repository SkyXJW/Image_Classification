# CIFAR-10 CNNå›¾åƒåˆ†ç±»å™¨

åŸºäºResNeté£æ ¼çš„CNNç½‘ç»œï¼Œç”¨äºCIFAR-10æ•°æ®é›†çš„å›¾åƒåˆ†ç±»ä»»åŠ¡ï¼Œæ”¯æŒ8å¼ NVIDIA 3090 GPUçš„åˆ†å¸ƒå¼è®­ç»ƒã€‚

## é¡¹ç›®ç»“æ„

```
.
â”œâ”€â”€ model.py                 # CNNæ¨¡å‹å®šä¹‰ï¼ˆResidualBlock + CIFAR10CNNï¼‰
â”œâ”€â”€ data_loader.py           # CIFAR-10æ•°æ®åŠ è½½å’Œé¢„å¤„ç†
â”œâ”€â”€ trainer.py               # åˆ†å¸ƒå¼è®­ç»ƒå™¨
â”œâ”€â”€ train.py                 # ä¸»è®­ç»ƒè„šæœ¬
â”œâ”€â”€ visualize.py             # è®­ç»ƒæ›²çº¿å¯è§†åŒ–
â”œâ”€â”€ requirements.txt         # ä¾èµ–åŒ…åˆ—è¡¨
â”œâ”€â”€ setup_env.sh             # Condaç¯å¢ƒåˆ›å»ºè„šæœ¬
â”œâ”€â”€ train_distributed.sh     # 8 GPUåˆ†å¸ƒå¼è®­ç»ƒå¯åŠ¨è„šæœ¬
â”œâ”€â”€ train_single_gpu.sh      # å•GPUè®­ç»ƒè„šæœ¬
â””â”€â”€ README.md                # æœ¬æ–‡ä»¶
```

## ç¯å¢ƒé…ç½®

### æ–¹æ³•1: ä½¿ç”¨è„šæœ¬è‡ªåŠ¨åˆ›å»ºç¯å¢ƒ

```bash
# åˆ›å»ºå¹¶é…ç½®condaç¯å¢ƒ
bash setup_env.sh

# æ¿€æ´»ç¯å¢ƒ
conda activate cifar10_cnn
```

### æ–¹æ³•2: æ‰‹åŠ¨åˆ›å»ºç¯å¢ƒ

```bash
# åˆ›å»ºcondaç¯å¢ƒ
conda create -n cifar10_cnn python=3.10 -y

# æ¿€æ´»ç¯å¢ƒ
conda activate cifar10_cnn

# å®‰è£…ä¾èµ–
pip install -r requirements.txt -i https://pypi.tuna.tsinghua.edu.cn/simple
```

## æ¨¡å‹æ¶æ„

### ç½‘ç»œç»“æ„

- **è¾“å…¥**: [B, 3, 32, 32] RGBå›¾åƒ
- **åˆå§‹å·ç§¯**: 3 â†’ 64é€šé“
- **æ®‹å·®å±‚1**: 64 â†’ 64 (2ä¸ªæ®‹å·®å—)
- **æ®‹å·®å±‚2**: 64 â†’ 128 (2ä¸ªæ®‹å·®å—, stride=2ä¸‹é‡‡æ ·)
- **æ®‹å·®å±‚3**: 128 â†’ 256 (2ä¸ªæ®‹å·®å—, stride=2ä¸‹é‡‡æ ·)
- **æ®‹å·®å±‚4**: 256 â†’ 512 (2ä¸ªæ®‹å·®å—, stride=2ä¸‹é‡‡æ ·)
- **å…¨å±€å¹³å‡æ± åŒ–**: [B, 512, 4, 4] â†’ [B, 512]
- **Dropout**: 0.5
- **å…¨è¿æ¥å±‚**: 512 â†’ 10
- **è¾“å‡º**: [B, 10] åˆ†ç±»logits

### è¶…å‚æ•°é…ç½®

| å‚æ•° | å€¼ | è¯´æ˜ |
|------|-----|------|
| Batch Size | 128 Ã— 8 = 1024 | æ¯GPU 128ï¼Œæ€»å…±1024 |
| Learning Rate | 0.1 | åˆå§‹å­¦ä¹ ç‡ |
| Optimizer | SGD | åŠ¨é‡0.9ï¼Œæƒé‡è¡°å‡5e-4 |
| LR Scheduler | CosineAnnealing | ä½™å¼¦é€€ç«åˆ°0 |
| Epochs | 200 | è®­ç»ƒè½®æ•° |
| Dropout | 0.5 | åˆ†ç±»å™¨å‰çš„Dropout |
| æ··åˆç²¾åº¦ | å¯ç”¨ | ä½¿ç”¨AMPåŠ é€Ÿè®­ç»ƒ |

## è®­ç»ƒ

### è®­ç»ƒè¿›åº¦æ˜¾ç¤º

è®­ç»ƒè¿‡ç¨‹ä¸­ä¼šæ˜¾ç¤ºï¼š
- ç¾è§‚çš„è®­ç»ƒé…ç½®ä¿¡æ¯è¡¨æ ¼
- æ¯ä¸ªepochçš„å®æ—¶è¿›åº¦æ¡ï¼ˆæ˜¾ç¤ºå½“å‰loss/accuracyï¼‰
- æ¯ä¸ªepochå®Œæˆåçš„è¯¦ç»†ä¿¡æ¯ï¼ˆLossã€Accuracyã€Best Accuracyã€Learning Rateï¼‰
- ğŸŒŸ æ ‡è®°è¡¨ç¤ºæ–°çš„æœ€ä½³æ¨¡å‹
- è®­ç»ƒå®Œæˆåçš„æ€»ç»“ä¿¡æ¯

### 8 GPUåˆ†å¸ƒå¼è®­ç»ƒï¼ˆæ¨èï¼‰

```bash
# ç¡®ä¿åœ¨cifar10_cnnç¯å¢ƒä¸­
conda activate cifar10_cnn

# å¯åŠ¨8 GPUè®­ç»ƒ
bash train_distributed.sh
```

### å•GPUè®­ç»ƒ

```bash
# ç¡®ä¿åœ¨cifar10_cnnç¯å¢ƒä¸­
conda activate cifar10_cnn

# å¯åŠ¨å•GPUè®­ç»ƒ
bash train_single_gpu.sh
```

### è‡ªå®šä¹‰è®­ç»ƒå‚æ•°

```bash
# ä½¿ç”¨torchrunå¯åŠ¨ï¼ˆ8 GPUï¼‰
torchrun --nproc_per_node=8 train.py \
    --data_dir ./cifar-10-batches-py \
    --batch_size 128 \
    --epochs 200 \
    --lr 0.1 \
    --num_workers 4 \
    --save_dir ./outputs

# å•GPUè®­ç»ƒ
python train.py \
    --data_dir ./cifar-10-batches-py \
    --batch_size 128 \
    --epochs 200 \
    --lr 0.1 \
    --num_workers 4 \
    --save_dir ./outputs
```

## è¾“å‡º

è®­ç»ƒå®Œæˆåï¼Œåœ¨`./outputs`ç›®å½•ä¸‹ä¼šç”Ÿæˆï¼š

- `cifar10_cnn_best.pth`: æœ€ä½³æ¨¡å‹æƒé‡
- `cifar10_cnn_latest.pth`: æœ€æ–°æ¨¡å‹æƒé‡
- `training_curves.png`: è®­ç»ƒLosså’Œæµ‹è¯•Accuracyæ›²çº¿å›¾

## æ•°æ®é›†

CIFAR-10æ•°æ®é›†åº”ä½äº`./cifar-10-batches-py`ç›®å½•ï¼ŒåŒ…å«ï¼š

- `data_batch_1` ~ `data_batch_5`: è®­ç»ƒæ•°æ®ï¼ˆå…±50000å¼ ï¼‰
- `test_batch`: æµ‹è¯•æ•°æ®ï¼ˆ10000å¼ ï¼‰
- `batches.meta`: å…ƒæ•°æ®

## æ€§èƒ½ä¼˜åŒ–

- **åˆ†å¸ƒå¼æ•°æ®å¹¶è¡Œ**: ä½¿ç”¨PyTorch DDPåœ¨8å¼ GPUä¸Šå¹¶è¡Œè®­ç»ƒ
- **æ··åˆç²¾åº¦è®­ç»ƒ**: ä½¿ç”¨AMPå‡å°‘æ˜¾å­˜å ç”¨å¹¶åŠ é€Ÿè®­ç»ƒ
- **æ•°æ®å¢å¼º**: éšæœºè£å‰ªå’Œæ°´å¹³ç¿»è½¬æå‡æ³›åŒ–èƒ½åŠ›
- **å­¦ä¹ ç‡è°ƒåº¦**: ä½™å¼¦é€€ç«ç­–ç•¥ä¼˜åŒ–æ”¶æ•›
- **æ®‹å·®è¿æ¥**: æ”¯æŒæ›´æ·±çš„ç½‘ç»œç»“æ„

## ä¾èµ–

- Python >= 3.10
- PyTorch >= 2.0.0
- torchvision >= 0.15.0
- numpy >= 1.24.0
- matplotlib >= 3.7.0
- tqdm >= 4.65.0

## è®¸å¯

æœ¬é¡¹ç›®ä»…ä¾›å­¦ä¹ å’Œç ”ç©¶ä½¿ç”¨(Students Help Students)ã€‚
